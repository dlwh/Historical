%
% File acl2010.tex
%
% Contact: nasmith@cs.cmu.edu

\documentclass[11pt,a4paper]{article}
\usepackage{acl2010}
\usepackage{times}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{latexsym}
%\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Reconstruction of Historical Cognate Groups Using Finite State Automata}

\author{}

\date{}

\begin{document}
\maketitle
\begin{abstract}
\end{abstract}
\section{Introduction}
\section{Background}
\subsection{Weighted Automata}

We encode distributions over strings as weighted finite state
automata ( XXX cite Mohri). Weighted automata have been successfully
applied to speech processing ( XXX cite Mohri) and more recently
to morphology (cite Dreyer and Eisner). Informally, a weighted
automaton encodes a distribution over strings (or pairs of strings)
as weighted paths through a graph. Each edge in the graph has a
label (a single character, or the empty character) and a weight.
The weight of a string is then the sum of all paths through the
graph that accept a string.

More specifically, a weighted transducer assigns weights from a set
$S$ to pairs of strings with characters in alphabets $\Sigma$ and
$\Delta$. Each path through the transducer corresponds to an alignment
between two strings, with an associated score for that alignment.
The weights in S must satisfy the semiring axioms, namely that there
is an operation $\oplus$ (addition) with an identity element $\bar
0$, an operation $\otimes$ with identity $\bar 1$ that distributes
over $\oplus$, and $\forall s\in S, s\otimes \bar 0 = \bar 0 \otimes
s = \bar 0$. Examples of semirings include the nonnegative real
numbers, the logarithms of the nonnegative reals, and the nonnegative
real numbers with $\otimes = +$ and $\oplus = \max$. These semirings
correspond to probabilities, log probabilities, and the Viterbi
approximation to log probabilities.

Formally, a weighted transducer over pairs of strings and a semiring
$(S,\oplus,\otimes,\bar 0, \bar 1)$ is a tuple
$(\Sigma,\Delta,Q,I,F,E,\lambda,\rho)$, where $\Sigma$ is the
``input'' alphabet, $\Delta$ the ``output'' alphabet, $Q$ a set of
states, $I \subseteq Q$ a set of initial states, $F \subseteq Q$ a
set of final states, $E \subseteq (Q \times Q \times \Sigma^+ \times
\Delta^+ \times S)$ the set of labeled and weighted edges, $\lambda:
I \rightarrow S$ an initial weight function, and $\rho: F \rightarrow
S$ the final weight function. A weighted acceptor can be viewed as
a transducer over a single alphabet, where the input and output
labels are always the same thing.

In our setting, we are primarily concerned with transducers and
acceptors over the log semiring. In this case, a transducer assigns
a (possibly unnormalized) log-probability to pairs of strings, and
an acceptor similarly assigns log-probabilities to single strings.

\subsection{Transducer Operations}

For our purposes we are concerned with three fundamental operations
on weighted transducers, all explained in more depth in (XXX cite
Mohri) The first is computing the sum of all paths through a
transducer, which corresponds to computing the partition function
of the distribution. This operation can be performed in worst case
cubic time (using a generalization of the Floyd-Warshall Algorithm),
though for acyclic or feed-forward transducers this can be improved
dramatically by using a generalization of Djisktra's algorithm, the
Bellman-Ford algorithm or other related algorithms.

The second is the composition of two transducers. Intuitively,
composition creates a new transducer that takes the output from one
automata and process it through the other transducer and returns
the output of that transducer. That is, consider two transducers
$T_1$ and $T_2$. $T_1$ has input alphabet $\Sigma$ and output
alphabet $\Delta$, while $T_2$ has input alphabet $\Delta$ and
output alphabet $\Omega$. The composition $T_1 \circ T_2$ returns
a new transducer over $\Sigma$ and $\Omega$ such that $(T_1 \circ
T_2)(s,t) = \bigoplus_{u} T_1(s,u)\otimes T_2(u,t)$. In this paper,
we use composition for marginalization. Given a factor $f_1(s,u;T_1)$
and another factor $f_2(u,t;T_2)$, composition corresponds to the
operation $\psi(s,t) = \sum_u f_1(s,u) f_2(u,t)$.

The third operation is transducer minimization. Transducer compositions
produces $O(nm)$ states, where $n$ and $m$ are the number of states
in each transducer. Repeated compositions compound the problem:
iterated composition of $k$ transducers produces $O(n^k)$ states.
Minimization alleviates this problem by collapsing indistinguishable
states into a single state.  Unfortunately, minimization does not
always shrink enough states. In section XXX, we discuss approaches
to ``lossy'' minimization that produce automata that are not exactly
the same, but are typically much smaller.

\section{Datasets}

In our experiments, we focus on two datasets. One is an automatically
generated word list of XXX (\#) cognate words from three Romance
languages (Portuguese, Italian and Spanish), as well as their common
ancestor Latin. (XXX more on where this came from). The other is a
hand-curated list of cognates words from Austronesian languages.
Both datasets are transcribed as IPA (Romance automatically,
Austronesian manually). (XXX More on who curated it ,etc)

Beyond the differences in human supervision, there are several
important differences between the datasets. First, the Oceanic
dataset is comprised of many more languages (XXX), but it is much
sparser: the Romance dataset has a word for every language in each
cognate group, while the Oceanic dataset has only XXX of possible
entries attested.

\section{Model}

In this section we present a generative model of the derivation of
cognate words. Our task is to recover both which words are cognate
with one another and to recover ancestral forms of those words.

We make a few high-level modeling assumptions. First, all words in
a cognate group have a single common ancestor word in a single
ancestral language. Second, each surface form in each language
evolved from its parent form via a regular stochastic process that
is shared across all cognate groups. Specifically, each branch in
the phylogenetic tree has a weighted finite state transducer from
parent strings to child strings.

With these principles in mind, we define a generative model of the cognate groups and their individual word forms in each language as follows:
\begin{enumerate}
  \item Choose a (binary) tree topology over languages.
  \item For each edge in the tree, choose a transducer $\phi_{a,d}$ over strings in an ancestor language $a$ and descendent language $d$.
  \item For each cognate group $g$:
    \begin{enumerate}
      \item Choose a root word form $a_{g\mathrm{ROOT}}$ from an automaton $\phi^0$.
      \item For each interior child $d$ with ancestor $a$:
        \begin{enumerate}
          \item Generate ancestral word $a_{gd} \sim p(\cdot|\phi_{a,d},a_{ga},a_{gd})$.
        \end{enumerate}
    \end{enumerate}
  \item For each word position $i$ in language $\ell$:
    \begin{enumerate}
      \item Choose a cognate group $z_{\ell i}$ uniformly at random. 
      \item Choose $w_{\ell i} \sim p(\cdot|\phi_{Pa(\ell),\ell},a_{z_{\ell i}Pa(\ell)},w_{\ell i})$.
    \end{enumerate}
\end{enumerate}

In our setting, we observe only the binary tree topology and the
words in each language, but (typically) not their ancestral forms,
the groups they belong to, nor the transducers that govern how the
words evolve. 

\section{Inference}

In our setting, we are given a set of languages and a list of words
in each language, and our objective is to determine which words are
cognate with each other words. In the romance dataset, we have the
additional constraint that each cognate group supports exactly one
word from each language. In effect, in the Romance dataset the 
inference task is reduced to finding a permutation of the respective
word lists to maximize the log probability of the observed words:
\begin{equation}
  \begin{split}
    \vec{\pi} = \arg\!\max_{\vec \pi} \sum_{g} p(\vec w_{(\ell,\pi_\ell(i))}|\vec \phi,\vec \pi)
   \end{split}
 \end{equation}
Maximizing this equation directly is (XXX I believe it's NP hard,
but I should check), and so instead we use a coordinate ascent
algorithm to iteratively maximize one permutation while holding the
others fixed:
\begin{equation}
  \begin{split}
    \pi_\ell = \arg\!\max_{\pi_\ell} \sum_{g} p(\vec w_{(\ell,\pi_\ell(i))}|\vec \phi,\vec \pi_{-\ell},\pi_\ell)
  \end{split}
\end{equation}
Each iteration is then actually an instance of bipartite graph
matching, with the words in one language one set of nodes, and the
current cognate groups in the other languages the other set of
nodes, and the edge weights between these nodes are the conditional
probability of each word belonging to each cognate group. Given these
weights, we use the greedy Competitive Linking (XXX cite) algorithm
to find an approximate maximizer.

One important note is initialization. In our early experiments we
found that choosing a random starting configuration usually led to
rather poor local optima. Instead, we started with empty trees, and
added one language in per iteration until all languages were added,
and then continued iterations on the full tree.

\subsection{Message Approximation}

Under the iterative bipartite algorithm, inference in each tree
computes the marginal distribution at

Unfortunately, the maximal number of states in a message in each
cognate group is exponential in the number of words assigned to
that group. Minimization can only help so much: in order for two
states to be collapsed, the distribution over transitions from those
states must be indistinguishable, or indistinguishable to within
some tolerance. In practice, for the kinds of transducers generated
in our model, minimization removes at most half the states, which is
not sufficient to negate the exponential growth.

Previous authors have focused on a combination of n-best lists and
unigram back-off models. (XXX cite eisner) In our setting, n-best
lists seem inadequate: a 10,000-best list for a typical message
only accounts for 50\% of message log perplexity. (XXX probably
should run experiments for comparison.) That is, the posterior
marginals in our model are fairly flat.

XXX this is old:

We consider a different approach based on state splitting. Intuitively,
we should prefer to use a unigram model over words unless there is
evidence that a particular sequence is more probable than that
independence assumption should suggest.

Using an expectation semiring (XXX cite Eisner), we tally the
expected number of transitions with a given label, as well as the
expected number of bigram transitions. Then, for each character y
(plus a hallucinated ``start'' character), we see if the distribution
over unigram transitions $p(x_t)$ is an adequate approximation of
the conditional distribution of unigrams $p(x_{t+1}|x_t = y)$. If
so, then the $y$ transition can be a self-loop into the basic unigram
model. If not, then we can create a new state which is transitioned
into after observing any $y$. To determine whether or not to make
a split, we measure the KL divergence between the distribution
$p(x_t)$ and $p(x_{t+1}|x_t=y)$, and use a threshold to determine
which states to merge and which to split.\footnote{It should be
noted that this form of the KL divergence is identical to the mutual
information.}

This state splitting approach lends itself to recursive invocation.
Once we consider bigrams, we could continue by splitting bigram
states, therefore creating trigram states, and so on. In our experiments,
we tried XXX, and found that 

XXX end old stuff

\section{Learning}

Thus far we have treated the transducer parameters $\phi$ as given.
In practice, we have  

Estimating the parameters to each transducer can be performed
analogously to the message approximation algorithm presented in the
previous section. For each pair of parent/child languages $(a,d)$
and each cognate group $g$, we maintain a factor marginal as a
transducer. Our goal is to compress these G factor marginals into
a single transducer that best approximates the alignments seen
across all cognate groups. We follow a similar approach to the
approach described for messages, but we must tally expected ``alignment
n-grams'' rather than character n-grams.

\section{Experiments}

\subsection{Baseline}

\subsection{Experiment 1: Romance}

XXX some text


For evaluation, we report two metrics. The first is pairwise accuracy
for each pair of languages averaged across pairs. The other
is accuracy measured in terms of the number of correctly and
completely reconstructed cognate groups. 

Table \ref{tbl:exp1} shows the results under various configurations. XXX

\begin{table}
  \begin{tabular}{|c|c|c|c|}
    Transducers & Messages & Pair Acc. & Rec. Acc.\\
    \hline
    \hline
    Levenshtein&Unigrams & & \\
    Levenshtein&Pos. Unigrams & & \\
    Levenshtein&Bigrams & & \\
    Levenshtein&K-Best & & \\
    \hline
    Learned&Unigrams & & \\
    Learned&Pos. Unigrams & & \\
    Learned&Bigrams & & \\
    Learned&K-Best & & \\
  \end{tabular}
  \caption{Accuracies for reconstructing cognate groups. Levenshtein
  refers to fixed parameter edit distance transducers with
  deletion/insertion and substitution costs set to XXX, respectively.
  Learned refers to automatically learned edit distances.}
  \label{tbl:exp1}
\end{table}

\subsection{Experiment 2: Austronesian}

\subsection{Experiment 3: Reconstructions}


\section{Previous Work}
XXX expand and cite:

1) The reconstruction engine: a computer implementation of the
comparative method\\
  A deterministic approach from 1994 designed to help linguists look
at reconstructions. Closer to Alex's stuff. \\
2) Automatic Detection of Orthographic Cues for Cognate Recognition (2006) \\
  Given a list of english/german words (translations of each other),
are they cognate? Pretty crappy paper.\\
3) Identifying Cognates by Phonetic and Semantic Similarity \\
  The most similar. Given a list of multiple words from four Native
American languages, sort out which words are cognate. Uses their
English glosses to look up WordNet distance in addition to phonetic
distances. Phonetic distances are measured either using DICE on
bigrams, Longest Common Subsequence, and then a kind of edit distance
on phone classes (that was earlier work he borrowed. ALINE) *DICE on
bigrams and LCS look like good baselines.*\\
  -- This approach was geared towards precision/recall, and in some
sense is the real task. I'll see if I can find the data. \\
4)  Combining Evidence in Cognate Identification (2004) \\
  A very similar paper to (3). Uses ALINE to do phonetic comparisons.
Doesn't compute proto-projections like we do. Direct comparison of
words. Performance in the 50-65\% Average Precision range.\\
5) Minimally Supervised Morphological Analysis by Multimodal Alignment (2000) \\
  Basically does what Eisner and his student did on pairwise entries,
so no graphical model.\\
6) Semi-Supervised Learning of Partial Cognates using Bilingual Bootstrapping
  Basically context driven. Different task, but probably worth mentioning.

\section{Conclusion}
\end{document}

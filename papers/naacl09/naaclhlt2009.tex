%
% File naaclhlt2009.tex
%
% Contact: nasmith@cs.cmu.edu

\documentclass[11pt]{article}
\usepackage{naaclhlt2009}
\usepackage{times}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{}

\author{}

\date{}

\begin{document}
\maketitle
\begin{abstract}
\end{abstract}
\section{Introduction}
\section{Background}
\subsection{Weighted Automata}

We encode distributions over strings as weighted finite state
automata ( XXX cite Mohri). Weighted automata have been successfully
applied to speech processing ( XXX cite Mohri) and more recently
to morphology (cite Dreyer and Eisner). Informally, a weighted
automaton encodes a distribution over strings (or pairs of strings)
as weighted paths through a graph. Each edge in the graph has a
label (a single character, or the empty character) and a weight.
The weight of a string is then the sum of all paths through the
graph that accept a string.

More specifically, a weighted transducer assigns weights from a set
$S$ to pairs of strings with characters in alphabets $\Sigma$ and
$\Delta$. Each path through the transducer corresponds to an alignment
between two strings, with an associated score for that alignment.
The weights in S must satisfy the semiring axioms, namely that there
is an operation $\oplus$ (addition) with an identity element $\bar
0$, an operation $\otimes$ with identity $\bar 1$ that distributes
over $\oplus$, and $\forall s\in S, s\otimes \bar 0 = \bar 0 \otimes
s = \bar 0$. Examples of semirings include the nonnegative real
numbers, the logarithms of the nonnegative reals, and the nonnegative
real numbers with $\otimes = +$ and $\oplus = \max$. These semirings
correspond to probabilities, log probabilities, and the Viterbi
approximation to log probabilities.

Formally, a weighted transducer over pairs of strings and a semiring
$(S,\oplus,\otimes,\bar 0, \bar 1)$ is a tuple
$(\Sigma,\Delta,Q,I,F,E,\lambda,\rho)$, where $\Sigma$ is the
``input'' alphabet, $\Delta$ the ``output'' alphabet, $Q$ a set of
states, $I \subseteq Q$ a set of initial states, $F \subseteq Q$ a
set of final states, $E \subseteq (Q \times Q \times \Sigma^+ \times
\Delta^+ \times S)$ the set of labeled and weighted edges, $\lambda:
I \rightarrow S$ an initial weight function, and $\rho: F \rightarrow
S$ the final weight function. A weighted acceptor can be viewed as
a transducer over a single alphabet, where the input and output
labels are always the same thing.

In our setting, we are primarily concerned with transducers and
acceptors over the log semiring. In this case, a transducer assigns
a (possibly unnormalized) log-probability to pairs of strings, and
an acceptor similarly assigns log-probabilities to single strings.

\subsection{Transducer Operations}

For our purposes we are concerned with three fundamental operations
on weighted transducers, all explained in more depth in (XXX cite
Mohri) The first is computing the sum of all paths through a
transducer, which corresponds to computing the partition function
of the distribution. This operation can be performed in worst case
cubic time (using a generalization of the Floyd-Warshall Algorithm),
though for acyclic or feed-forward transducers this can be improved
dramatically by using a generalization of Djisktra's algorithm, the
Bellman-Ford algorithm or other related algorithms.

The second is the composition of two transducers. Intuitively,
composition creates a new transducer that takes the output from one
automata and process it through the other transducer and returns
the output of that transducer. That is, consider two transducers
$T_1$ and $T_2$. $T_1$ has input alphabet $\Sigma$ and output
alphabet $\Delta$, while $T_2$ has input alphabet $\Delta$ and
output alphabet $\Omega$. The composition $T_1 \circ T_2$ returns
a new transducer over $\Sigma$ and $\Omega$ such that $(T_1 \circ
T_2)(s,t) = \bigoplus_{u} T_1(s,u)\otimes T_2(u,t)$. In this paper,
we use composition for marginalization. Given a factor $f_1(s,u;T_1)$
and another factor $f_2(u,t;T_2)$, composition corresponds to the
operation $\psi(s,t) = \sum_u f_1(s,u) f_2(u,t)$.

The third operation is transducer minimization. Transducer compositions
produces $O(nm)$ states, where $n$ and $m$ are the number of states
in each transducer. Repeated compositions compound the problem:
iterated composition of $k$ transducers produces $O(n^k)$ states.
Minimization alleviates this problem by collapsing indistinguishable
states into a single state.  Unfortunately, minimization does not
always shrink enough states. In section XXX, we discuss approaches
to ``lossy'' minimization that produce automata that are not exactly
the same, but are typically much smaller.

\section{Model}

In this section we present a generative model of the derivation of
cognate words. Our task is to recover both which words are cognate
with one another and to recover ancestral forms of those words.

We make a few high-level modeling assumptions. First, all words in
a cognate group have a single common ancestor word in a single
ancestral language. Second, each surface form in each language
evolved from its parent form via a regular stochastic process that
is shared across all cognate groups. Specifically, each branch in
the phylogenetic tree has a weighted finite state transducer from
parent strings to child strings.

With these principles in mind, we define a generative model of the cognate groups and their individual word forms in each language as follows:
\begin{enumerate}
  \item Choose a (binary) tree topology over languages.
  \item For each edge in the tree, choose a transducer $\phi_{a,d}$ over strings in an ancestor language $a$ and descendent language $d$.
  \item For each cognate group $g$:
    \begin{enumerate}
      \item Choose a root word form $a_{g\mathrm{ROOT}}$ from an automaton $\phi^0$.
      \item For each interior child $d$ with ancestor $a$:
        \begin{enumerate}
          \item Generate ancestral word $a_{gd} \sim \phi_{a,d}(a_{ga},a_{gd})$.
        \end{enumerate}
    \end{enumerate}
  \item For each word position $i$ in language $\ell$:
    \begin{enumerate}
      \item Choose a cognate group $z_{\ell i}$ uniformly at random. (XXX: will probably want a DP over this)
      \item Choose $w_{\ell i} \sim \phi_{Pa(\ell),\ell}(a_{z_{\ell i}Pa(\ell)},w_{\ell i})$.
    \end{enumerate}
\end{enumerate}

In our setting, we observe only the binary tree topology and the words in each language, but not their ancestral forms, the groups they belong to, or the transducers that
govern how the words evolve.  In order to
estimate these parameters we use a variant of ``collapsed'' simulated annealing (XXX cite), in which we explicitly sample only the cognate group to which each word belongs. 
We estimate the parameters for the transducers $\phi$ based on these samples, which we discuss in section XXX.

The parameters XXX
\begin{equation}
  \begin{split}
    p(\vec w, \vec z, &\vec a, \mathbf{\phi}) = (\prod_\ell \prod_i p(w_{\ell
    i}|z_{\ell i},a_{g_{\ell i}Pa(\ell)},\phi_{Pa(\ell),\ell})p(z_{\ell i}))\\
      & (\prod_g p(\vec a_g|\mathbf{\phi}) ) p(\mathbf{\phi})
   \end{split}
 \end{equation}

 XXX

\section{Message Approximation}

Unfortunately, the maximal number of states in a message in each
cognate group is exponential in the number of words assigned to
that group. Minimization can only help so much: in order for two
states to be collapsed, the distribution over transitions from those
states must be indistinguishable, or indistinguishable to within
some tolerance. In practice, for the kinds of transducers generated
in our model, minimization removes at most half the states, which is
not sufficient to negate the exponential growth.

Previous authors have focused on a combination of n-best lists and
unigram back-off models. (XXX cite eisner) In our setting, n-best
lists seem inadequate: a 10,000-best list for a typical message
only accounts for 50\% of message log perplexity. (XXX probably
should run experiments for comparison.)

We consider a different approach based on state splitting. Intuitively,
we should prefer to use a unigram model over words unless there is
evidence that a particular sequence is more probable than that
independence assumption should suggest.

Using an expectation semiring (XXX cite Eisner), we tally the
expected number of transitions with a given label, as well as the
expected number of bigram transitions. Then, for each character y
(plus a hallucinated ``start'' character), we see if the distribution
over unigram transitions $p(x_t)$ is an adequate approximation of
the conditional distribution of unigrams $p(x_{t+1}|x_t = y)$. If
so, then the $y$ transition can be a self-loop into the basic unigram
model. If not, then we can create a new state which is transitioned
into after observing any $y$. To determine whether or not to make
a split, we measure the KL divergence between the distribution
$p(x_t)$ and $p(x_{t+1}|x_t=y)$, and use a threshold to determine
which states to merge and which to split.\footnote{It should be
noted that this form of the KL divergence is identical to the mutual
information.}

This state splitting approach lends itself to recursive invocation.
Once we consider bigrams, we could continue by splitting bigram
states, therefore creating trigram states, and so on. In our experiments,
we tried XXX, and found that 

\subsection{Transducer Estimation}

Estimating the parameters to each transducer can be performed
analogously to the message approximation algorithm presented in the
previous section. For each pair of parent/child languages $(a,d)$
and each cognate group $g$, we maintain a factor marginal as a
transducer. Our goal is to compress these G factor marginals into
a single transducer that best approximates the alignments seen
across all cognate groups. We follow a similar approach to the
approach described for messages, but we must tally expected ``alignment
n-grams'' rather than character n-grams.

XXX possibly featurize.

\section{Experiments}
\section{Conclusion}
\end{document}

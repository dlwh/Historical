%
% File naaclhlt2009.tex
%
% Contact: nasmith@cs.cmu.edu

\documentclass[11pt]{article}
\usepackage{naaclhlt2009}
\usepackage{times}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{}

\author{}

\date{}

\begin{document}
\maketitle
\begin{abstract}
\end{abstract}
\section{Introduction}
\section{Model}

We make a few high-level modeling assumptions. First, all words in
a cognate group have a single common ancestor word in a single
ancestral language. Second, each surface form in each language
evolved from its parent form via a regular stochastic process that
is shared across all cognate groups. Specifically, each branch in
the phylogenetic tree has a weighted finite state transducer from
parent strings to child strings.

With these principles in mind, we define a generative model of the cognate groups and their individual word forms in each language as follows:
\begin{enumerate}
  \item Choose a (binary) tree topology over languages.
  \item For each edge in the tree, choose a transducer $\phi_{a,d}$ over strings in an ancestor language $a$ and descendent language $d$.
  \item For each cognate group $g$:
    \begin{enumerate}
      \item Choose a root word form $a_{g\mathrm{ROOT}}$ uniformly over strings. (XXX This is degenerate. Oh well.)
      \item For each interior child $d$ with ancestor $a$:
        \begin{enumerate}
          \item Generate ancestral word $a_{gd} \sim \phi_{a,d}(a_{ga},a_{gd})$.
        \end{enumerate}
    \end{enumerate}
  \item For each word position $i$ in language $\ell$:
    \begin{enumerate}
      \item Choose a cognate group $z_{\ell i}$ uniformly at random. (XXX: will probably want a DP over this)
      \item Choose $w_{\ell i} \sim \phi_{Pa(\ell),\ell}(a_{z_{\ell i}Pa(\ell)},w_{\ell i})$.
    \end{enumerate}
\end{enumerate}

In our setting, we observe only the binary tree topology and the words in each language, but not their ancestral forms, or even which cognate group they belong to. Estimating
the remaining parameters requires EM, which we derive from a variational prospective. The full data likelihood is given by:
\begin{equation}
  \begin{split}
    p(\vec w, \vec z, &\vec a, \mathbf{\phi}) = (\prod_\ell \prod_i p(w_{\ell
    i}|z_{\ell i},a_{g_{\ell i}Pa(\ell)},\phi_{Pa(\ell),\ell})p(z_{\ell i}))\\
      & (\prod_g p(\vec a_g|\mathbf{\phi}) ) p(\mathbf{\phi})
   \end{split}
 \end{equation}
 For simplicity, we assume (XXX probably won't want to do this) a uniform distribution over cognate group assignments $p(g_{\ell i })$, and a degenerate prior over
 transducers.
 We consider the factorized distribution 
 \begin{equation}
   \begin{split}
      q(\vec z, \vec a, \mathbf{\phi}) = (\prod_{a_1,a_2} q_{a_1,a_2}(\mathbf{\phi}_{a_1,a_2}))\prod_\ell \prod_i q_{\ell i}(z_{\ell i})\prod_g
 q_g(\vec a_g)
    \end{split}
  \end{equation}
 where we restrict the $q(\phi)$'s to be point-masses. We seek to
 minimize the KL divergence between $q$ and the posterior distribution $p(\vec z,
 \vec a, \mathbf{\phi}| \vec w)$. Doing so leads to the updates for each group
 assignments $z_{\ell i}$:
 \begin{equation}
   \begin{split}
     \log q_{\ell i}&(z_{\ell i}) \propto E_q[\log p(w_{\ell i}, z_{\ell i}, a_{z_{\ell i} Pa(\ell)},\mathbf{\phi})] + \mathrm{const}\\
     &= E_q[\log  p(w_{\ell i}|z_{\ell i},a_{z_{\ell i}Pa(\ell)},\phi_{Pa(\ell)\ell}) ] + \mathrm{const}\\
     &= \sum \phi^{\log}_{Pa(\ell)\ell}(a_{gPa(\ell)},w_{\ell i}) q(a_{gPa(\ell)}) + \mathrm{const}\\
     &= \psi^{\log}_{\ell g}(w_{\ell i}) + \mathrm{const}\\
    \end{split}
  \end{equation}
where $\phi^{\log}_{Pa(\ell)\ell}$ is the transducer
$\phi_{Pa(\ell)\ell}$ for transforming words in $\ell$'s parent
language into words in $\ell$ itself except that arc transitions
are replaced by their log-space counterparts. Each log probability
can computed using an expectation semiring (XXX cite Eisner) and a
variant of weighted transducer composition (XXX cite Mohri). In
fact, a log-space automaton over words $\psi^{\log}_{\ell g}(w)$
can be precomputed by running (modified) composition and then
projecting the resulting transducer to only consider words in the
surface language.

Now we consider the computation of $\log q_g(\vec a_g)$, the log
probability of the words in each ancestor language for a given
group. Note that since each group is a binary tree with interior
nodes being string-valued random variables, we can perform inference
easily inside the tree using the inside-outside algorithm (XXX
cite?). The observed words themselves, however, require using the
variational approximation $q_{\ell i}(z_{\ell i})$, as follows:
 \begin{equation}
   \begin{split}
     \log &q_{g}(\vec a_g) \propto E_q[\log p(\vec w, \vec z, \vec a_g,\mathbf{\phi})] + \mathrm{const}\\
     &= E_q[\log p(\vec a_g|\mathbf\phi) + \sum_{\ell,i}\log p(w_{\ell i}| z_{\ell i} = g, \vec a_g,\mathbf\phi)] + \mathrm{const}\\
     &= \log p(\vec a_g|\mathbf\phi) + \sum_{\ell,i} q(z_{\ell i}=g) \log p(w_{\ell i}|a_{g Pa(\ell)},\phi_{\ell,Pa(\ell)}) + \mathrm{const}\\
     &= \log p(\vec a_g|\mathbf\phi) + \sum_{\ell,i} q(z_{\ell i}=g) \phi_{\ell,Pa(\ell)}^{\log}(a_{g Pa(\ell)},w_{\ell i}) + \mathrm{const}
    \end{split}
  \end{equation}
Note that the right hand summands only depend only on the direct
parent of each language $\ell$, and so these can be used as a base
case in the inside pass of the algorithm for the entire tree.
Moreover, we can also precompute automata
$\phi_{\ell,Pa(\ell)}(\cdot,w_{\ell i})$ for each word. In the case
of a fixed topology for $\phi_{\ell,Pa(\ell)}$, these automata can
in fact be created once and reused in every iteration, changing
the weight of the automata as $\phi$ is updated.

For $\phi$, we (XXX how best to do $phi$? For the moment:XXX) have
G edges in each tree that are governed by any given $\phi_{ad}$,
for a given ancestor language $a$ and descendent language $d$. We
restrict $q(\phi_{ad})$ to be a point estimate $\delta(\phi_{ad})$,
which means that we wish to find:XXX fix up a,d confusion
\begin{equation}
  \begin{split}
    \phi_{ad}^* &= \arg\max_{\phi_{ad}^* \in \Phi} E_q[\log p(\vec w, \vec z, \vec a,\mathbf{\phi})] \\
    &= \arg\max_{\phi_{ad} \in \Phi} E_q[\log p(a_{ga},a_{gd}|\phi_{ad})]\\
    &= \arg\max_{\phi_{ad} \in \Phi} E_q[\phi^{\log}_{a,d}(a_{ga},a_{gd}))]
   \end{split}
 \end{equation}
which again can be computed using an expectation semiring. In this
paper, we explore several possible choices for the set of possible
transducers $\Phi$.
 (XXX with exterior nodes similarly, taking into account $q(z)$ XXX perhaps I should show this.)
\section{Experiments}
\section{Conclusion}
\end{document}
